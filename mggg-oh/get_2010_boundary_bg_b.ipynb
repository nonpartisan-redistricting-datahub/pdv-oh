{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # for plotting maps\n",
    "import pandas as pd # standard python data library\n",
    "import geopandas as gp # the geo-version of pandas\n",
    "import numpy as np\n",
    "import time\n",
    "import requests\n",
    "import os\n",
    "from urllib.request import urlopen\n",
    "import certifi\n",
    "import ssl\n",
    "from zipfile import ZipFile\n",
    "from datetime import date\n",
    "import shutil\n",
    "#ef8d7d2d71226a4e4f86b6ee741c8d8f979d6c7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/adrianarogers/Documents/datahub/partner_data_validation/pdv-oh/mggg-oh'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent_dir = os.getcwd()\n",
    "parent_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This function gets a file from a url and unzips in the current working directory'''\n",
    "\n",
    "def get_and_unzip(url, data_dir=os.getcwd()): #os.getcwd() puts the data_dir default as the current working directory\n",
    "    basename = url.split(\"/\")[-1] #gets the name of what will be the file and downloads it\n",
    "    name_with_path = os.path.join(data_dir, basename) #appends basename to the set working directory to know where the downloaded is housed\n",
    "    if not os.path.exists(name_with_path): #if the the data does does not exist in the directory, \n",
    "        #print('url: ', url)\n",
    "        file_data = urlopen(url, context=ssl.create_default_context(cafile=certifi.where()))\n",
    "        data_to_write = file_data.read()\n",
    "        with open(name_with_path, \"wb\") as f:\n",
    "            f.write(data_to_write)\n",
    "\n",
    "        zip_obj = ZipFile(name_with_path) #recognizes data downloaded as a zip file\n",
    "        zip_obj.extractall(data_dir) #extracts zipped data\n",
    "        del(zip_obj) #deletes the zipped folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This function retrieves the county fips for all counties in a state and returns them in a pandas dataframe.'''\n",
    "def counties(state_fips):\n",
    "    \"\"\"Inputs: state fips code\n",
    "    Process: Retrieves a list of counties in the given state from the Census API.  \n",
    "    Outputs: A list of county fips codes in the state. \"\"\"\n",
    "    #uses the fips input into the census api\n",
    "    resp = requests.get(\n",
    "        \"https://api.census.gov/data/2010/dec/sf1\"\n",
    "        \"?get=NAME&for=county:*&in=state:{}\".format(state_fips)  #uses the fips input to locate the state\n",
    "    )\n",
    "    #retrieves the data as a json \n",
    "    header, *rows = resp.json()\n",
    "    #county column is \"county\"\n",
    "    county_column_index = header.index(\"county\")\n",
    "    county_fips = set(row[county_column_index] for row in rows) #sequence of counties \n",
    "    county_name_index = header.index(\"NAME\")\n",
    "    county_names = set(row[county_name_index] for row in rows)\n",
    "    county_fips = np.array(list(county_fips))\n",
    "    county_names = np.array(list(county_names))\n",
    "    df = pd.DataFrame({'COUNTYFP10': county_fips, 'COUNTYNAMES': county_names}) #make pd dataframe of arrays\n",
    "    df['COUNTY_STATE_FIPS']=state_fips + df['COUNTYFP10']\n",
    "    return df #returns the fips codes of all counties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This function removes supporting geography files if they exist based on a name (input). It can remove a zip folder as well (optional input)'''\n",
    "def remove_geog_files(name, zip_remove = 'remove_zip'):\n",
    "    if os.path.exists(name+'.cpg'):\n",
    "        os.remove(name+'.cpg')\n",
    "    if os.path.exists(name+'.dbf'):\n",
    "        os.remove(name+'.dbf')\n",
    "    if os.path.exists(name+'.prj'):\n",
    "        os.remove(name+'.prj')\n",
    "    if os.path.exists(name+'.shp'):\n",
    "        os.remove(name+'.shp')\n",
    "    if os.path.exists(name+'.shp.ea.iso.xml'):\n",
    "        os.remove(name+'.shp.ea.iso.xml')\n",
    "    if os.path.exists(name+'.shp.iso.xml'):\n",
    "        os.remove(name+'.shp.iso.xml')\n",
    "    if os.path.exists(name+'.shx'):\n",
    "        os.remove(name+'.shx')\n",
    "    if os.path.exists(name+'.shp.xml'):\n",
    "        os.remove(name+'.shp.xml')\n",
    "    if os.path.exists(name+'.xml'):\n",
    "        os.remove(name+'.xml')\n",
    "    if zip_remove == 'remove_zip':\n",
    "        if os.path.exists(name+'.zip'):\n",
    "            os.remove(name+'.zip')\n",
    "    if zip_remove == 'keep_zip':\n",
    "        if os.path.exists(name+'.zip'):\n",
    "            print('Zipped file is: ', name+'.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This function takes a state FIPS code (input) and retrieves the postal code abbreviation for the state'''\n",
    "def assign_postalcode(fips):\n",
    "    values = ['01','02','04','05','06','08','09','10','12','13','15','16','17','18','19','20','21','22','23',\n",
    "                  '24','25','26','27','28','29','30','31','32','33','34','35','36','37','38','39','40','41','42','44','45','46',\n",
    "                  '47','48','49','50','51','53','54','55','56']\n",
    "    keys = ['al','ak','az','ar','ca','co','ct','de','fl','ga','hi','id','il','in','ia','ks','ky','la','me','md','ma','mi','mn','ms','mo','mt','ne','nv','nh','nj','nm','ny','nc','nd','oh','ok','or','pa','ri','sc','sd','tn','tx','ut','vt','va','wa','wv','wi','wy']\n",
    "    dictionary = dict(zip(keys,values))\n",
    "    state_ab = ''\n",
    "    for key, value in dictionary.items(): \n",
    "        if value == fips: \n",
    "            state_ab=key\n",
    "    return state_ab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This function takes a file name (without the file type) and zips it into a folder with supporting files given the type of zip file desired (shp or csv as inputs for dtype)'''\n",
    "def zip_folder(name,dtype):\n",
    "    zipObj = ZipFile(name+'.zip', 'w')\n",
    "    if dtype == 'csv':\n",
    "        zipObj.write(name+'.csv')\n",
    "        zipObj.close()\n",
    "        os.remove(name+'.csv')\n",
    "    if dtype == 'shp':\n",
    "        zipObj.write(name+'.cpg')\n",
    "        zipObj.write(name+'.dbf')\n",
    "        zipObj.write(name+'.prj')\n",
    "        zipObj.write(name+'.shp')\n",
    "        zipObj.write(name+'.shx')\n",
    "        zipObj.close()\n",
    "        remove_geog_files(name, 'keep_zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''*** This is the list of table numbers of strings that are to be pulled from the 2010 SF1 Decennial Census. This code can be modified to read in a dictionary if you want to do the renaming in the function as well.'''\n",
    "variables = [\n",
    "    # pop\n",
    "    \"P001001\",\n",
    "    \"P005003\",\n",
    "    \"P005004\",\n",
    "    \"P005005\",\n",
    "    \"P005006\",\n",
    "    \"P005007\",\n",
    "    \"P005008\",\n",
    "    \"P005009\",\n",
    "    \"P005010\",\n",
    "    \"P005011\",\n",
    "    \"P005012\",\n",
    "    \"P005013\",\n",
    "    \"P005014\",\n",
    "    \"P005015\",\n",
    "    \"P005016\",\n",
    "    \"P005017\",\n",
    "    # vap\n",
    "    \"P011001\",\n",
    "    \"P011002\",\n",
    "    \"P011005\",\n",
    "    \"P011006\",\n",
    "    \"P011007\",\n",
    "    \"P011008\",\n",
    "    \"P011009\",\n",
    "    \"P011010\",\n",
    "    \"P011011\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This function takes a state fips, a level of geography (either block 'b' or block group 'bg'), if/how the file is to be saved ('shp', 'csv', 'shp csv', else there is no save), and if the saved data is to be zipped or not ('zip' or else no zip).\n",
    "Optional inputs are the variables list (defaults to the list in the previous cell, the parent_dir (set at the beginning as the current working directory), and the Census API Key).\n",
    "The function retrieves census data from given input variables for a state/geog and then returns them as a geodataframe. Saving and zipping files are optional inputs.'''\n",
    "\n",
    "def get_census_data(fip, geog, save, zipf, variables=variables,parent_dir=parent_dir, CENSUS_API_KEY = 'ef8d7d2d71226a4e4f86b6ee741c8d8f979d6c7b'):\n",
    "    file_suffix = 'demographics'\n",
    "    folder_suffix = file_suffix + '_2010'\n",
    "    HOST = \"https://api.census.gov/data\"\n",
    "    # set year for data and acs5 or sf1 (sf1 stands for summary file 1)\n",
    "    # as of July, 2018 - documentation can be found\n",
    "    # here: https://www.socialexplorer.com/data/C2010/metadata/?ds=SF1\n",
    "    year = \"2010\"\n",
    "    dataset = \"dec/sf1\"\n",
    "    base_url = \"/\".join([HOST, year, dataset])\n",
    "    data = []\n",
    "    if 'NAME' not in variables:\n",
    "        variables.append('NAME')\n",
    "    counties_codes_TEST = counties(fip)\n",
    "    counties_fips = counties_codes_TEST['COUNTY_STATE_FIPS']\n",
    "    print('starting to collect data for ' + geog + ' ' + fip)\n",
    "    for county in counties_fips:\n",
    "    # for county_code in state_county_dict[fip]:\n",
    "        predicates = {}\n",
    "        predicates[\"get\"] = \",\".join(variables)\n",
    "        if geog == 'b':\n",
    "            predicates[\"for\"] = \"block:*\"\n",
    "        if geog == 'bg':\n",
    "            predicates[\"for\"] = \"block group:*\"\n",
    "        predicates[\"in\"] = \"state:\" + fip + \"+county:\" + county[2:]\n",
    "        predicates[\"key\"] = CENSUS_API_KEY\n",
    "            # Write the result to a response object:\n",
    "        response = requests.get(base_url, params=predicates)\n",
    "        col_names = response.json()[0]        \n",
    "        data = data + response.json()[1:]\n",
    "    print('done collecting data for', fip)\n",
    "    geoids = []  # initialize geoid vector\n",
    "    pop_data = pd.DataFrame(columns=col_names, data=data)\n",
    "    cols = [i for i in pop_data.columns if i not in [\"NAME\",\"state\",\"county\",\"tract\",'block group',\"block\"]]\n",
    "    for col in cols:\n",
    "        pop_data[col]=pd.to_numeric(pop_data[col])\n",
    "    for index, row in pop_data.iterrows():\n",
    "        # make changes here for tracts\n",
    "        if geog == 'b':\n",
    "            geoid = row[\"state\"] + row[\"county\"] + row[\"tract\"] + row[\"block\"]\n",
    "        if geog == 'bg':\n",
    "            geoid = row[\"state\"] + row[\"county\"] + row[\"tract\"] + row[\"block group\"]\n",
    "        geoids.append(geoid)\n",
    "    pop_data[\"GEOID\"] = geoids\n",
    "    pop_data.set_index([\"state\", \"county\", \"tract\"], drop=False, inplace=True)\n",
    "    if geog == 'b':\n",
    "        pop_data['COUNTY']=pop_data['NAME'].apply(lambda x: x.split(',')[3])\n",
    "        pop_data['STATE']=pop_data['NAME'].apply(lambda x: x.split(',')[4])\n",
    "        url_block = \"https://www2.census.gov/geo/tiger/\" \\\n",
    "            \"TIGER2010/TABBLOCK/2010/tl_2010_\" + fip + \"_tabblock10.zip\"\n",
    "    if geog == 'bg':\n",
    "        pop_data['COUNTY']=pop_data['NAME'].apply(lambda x: x.split(',')[2])\n",
    "        pop_data['STATE']=pop_data['NAME'].apply(lambda x: x.split(',')[3])\n",
    "        url_block = 'https://www2.census.gov/geo/tiger/GENZ2010/gz_2010_' + fip + '_150_00_500k.zip'\n",
    "    get_and_unzip(url_block, os.getcwd())\n",
    "    if geog == 'b':    \n",
    "        shp_geog = gp.read_file(\"tl_2010_\" + fip + \"_tabblock10.shp\")\n",
    "        shp_geog.rename(columns={'GEOID10':'GEOID','STATEFP10':'STATEFP','COUNTYFP10':'COUNTYFP','TRACTCE10':'TRACTCE','BLOCKCE10':'BLOCKCE','ALAND10':'ALAND','AWATER10':'AWATER'}, inplace=True)\n",
    "    if geog == 'bg':\n",
    "        shp_geog = gp.read_file('gz_2010_' + fip + '_150_00_500k.shp')\n",
    "        shp_geog['GEOID'] = shp_geog['GEO_ID'].str[9:]\n",
    "        shp_geog.rename(columns={'STATE':'STATEFP','COUNTY':'COUNTYFP','TRACT':'TRACTCE','BLKGRP':'BLKGRPCE','NAME':'x'},inplace=True)\n",
    "    print('starting merge of data and ' + geog +  ' for ' +  fip)\n",
    "    geog_data = pd.merge(shp_geog, pop_data, on= \"GEOID\")\n",
    "    variables.remove('NAME')\n",
    "    cols_to_keep=['GEOID','NAME','STATE','COUNTY'] + variables + ['STATEFP','COUNTYFP','TRACTCE','BLKGRPCE','BLOCKCE','ALAND','AWATER','geometry']\n",
    "    col_list = list(geog_data.columns)\n",
    "    #print(col_list)\n",
    "    new_cols = []\n",
    "    for i in col_list:\n",
    "        #print(i)\n",
    "        column = [x for x in cols_to_keep if i in x]\n",
    "        if len(column) > 0:\n",
    "            add_col = column[0]\n",
    "            #print(add_col)\n",
    "            new_cols.append(add_col)\n",
    "    geog_data = geog_data[new_cols]\n",
    "    print(geog_data.columns)\n",
    "    if geog == 'b':\n",
    "        reindex = ['GEOID','NAME','STATE','COUNTY'] + variables + ['STATEFP','COUNTYFP','TRACTCE','BLOCKCE','ALAND','AWATER','geometry']\n",
    "        print('reindex')\n",
    "        geog_data = geog_data[reindex]\n",
    "    if geog == 'bg':\n",
    "        reindex = ['GEOID','NAME','STATE','COUNTY'] + variables + ['STATEFP','COUNTYFP','TRACTCE','BLKGRPCE','geometry']\n",
    "        geog_data = geog_data[reindex]\n",
    "    directory = geog+'_'+folder_suffix\n",
    "    #print('parent_dir: ', parent_dir)\n",
    "    path = os.path.join(parent_dir, directory)\n",
    "    #print('folder path: ', path)\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "    os.chdir(path)\n",
    "    #print('current shp wd: ', os.getcwd())\n",
    "    state_ab = assign_postalcode(fip)\n",
    "    final_cols = geog_data.columns\n",
    "    name = state_ab + '_2010_' + geog + '_' + file_suffix\n",
    "    if save == 'shp':\n",
    "        geog_data.to_file(name+'.shp')\n",
    "        #zip_folder(name, 'shp')\n",
    "        print('done merging data and geography for', fip)\n",
    "        os.chdir(parent_dir)\n",
    "        if zipf == 'zip':\n",
    "            zip_folder(name,save)\n",
    "    if save == 'csv':\n",
    "        reindex.pop()\n",
    "        geog_data = geog_data[reindex]\n",
    "        geog_data = pd.DataFrame(geog_data)\n",
    "        geog_data.to_csv(name+'.csv')\n",
    "        geog_data.to_csv(name+'.csv',index=False) \n",
    "        if zipf == 'zip':\n",
    "            zip_folder(name,save)\n",
    "    if save == 'csv shp':\n",
    "        geog_data.to_file(name+'.shp')\n",
    "        print('done merging data and geography for', fip)\n",
    "        os.chdir(parent_dir)\n",
    "        if zipf == 'zip':\n",
    "            shp = 'shp'\n",
    "            zip_folder(name,shp)\n",
    "        reindex.pop()\n",
    "        geog_data = geog_data[reindex]\n",
    "        geog_data = pd.DataFrame(geog_data)\n",
    "        geog_data.to_csv(name+'.csv')\n",
    "        geog_data.to_csv(name+'.csv',index=False) \n",
    "        if zipf == 'zip':\n",
    "            csv = 'csv'\n",
    "            zip_folder(name,csv)\n",
    "    os.chdir(parent_dir)\n",
    "    if geog == 'b':\n",
    "        name = \"tl_2010_\" + fip + \"_tabblock10\"\n",
    "        remove_geog_files(name, 'remove_zip')\n",
    "    if geog == 'bg':\n",
    "        name = \"gz_2010_\" + fip + \"_150_00_500k\"\n",
    "        remove_geog_files(name, 'remove_zip')\n",
    "    return geog_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All states are in this FIPS list to iterate over\n",
    "fips = ['01','02','04','05','06','08','09','10','12','13','15','16','17','18','19','20','21','22','23',\n",
    "                  '24','25','26','27','28','29','30','31','32','33','34','35','36','37','38','39','40','41','42','44','45','46',\n",
    "                  '47','48','49','50','51','53','54','55','56']\n",
    "fips = ['39'] #ohio\n",
    "#These are the two possible values for geog\n",
    "geog = ['b','bg']\n",
    "geog = ['b']\n",
    "#Save has these three options and will either return no file outputs (the function returns a geodataframe), a csv, a shp, or both csv and shp\n",
    "save = 'shp'\n",
    "\n",
    "#You can change zipf to 'zip' and it will zip the outputs but probably doesn't make sense since you are manipulating the data after.\n",
    "zipf = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This function runs the get_census_data function for all states at the block and block group level and saves them as unzipped shapefiles.\n",
    "All inputs are optional and default to the defined variables in the previous cell.'''\n",
    "def run_2010_census(zipf = zipf, save=save,fips=fips,geog=geog):\n",
    "    for g in geog:\n",
    "        for f in fips:\n",
    "            print('NOW STARTING ', f, ' ', g)\n",
    "            get_census_data(f, g, save, zipf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW STARTING  39   b\n",
      "starting to collect data for b 39\n",
      "done collecting data for 39\n",
      "starting merge of data and b for 39\n",
      "Index(['STATEFP', 'COUNTYFP', 'TRACTCE', 'BLOCKCE', 'GEOID', 'ALAND', 'AWATER',\n",
      "       'geometry', 'P001001', 'P005003', 'P005004', 'P005005', 'P005006',\n",
      "       'P005007', 'P005008', 'P005009', 'P005010', 'P005011', 'P005012',\n",
      "       'P005013', 'P005014', 'P005015', 'P005016', 'P005017', 'P011001',\n",
      "       'P011002', 'P011005', 'P011006', 'P011007', 'P011008', 'P011009',\n",
      "       'P011010', 'P011011', 'NAME', 'COUNTY', 'STATE'],\n",
      "      dtype='object')\n",
      "reindex\n",
      "done merging data and geography for 39\n"
     ]
    }
   ],
   "source": [
    "run_2010_census()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
